{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506adcbf-af16-406e-9099-6db4855eff82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Modelagem de Linguagem Mascarada de Ponta a Ponta com BERT - Dataset IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d7b82-0f0b-4c0c-9ec1-cc8c2eb2fdb9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2488ccd-20f0-4951-a28d-8bf56c65c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define qual backend o Keras deve usar. Neste caso, está sendo forçado a utilizar o TensorFlow como backend.\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Importa os principais módulos da biblioteca Keras para criação de modelos e camadas.\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization  # Camada responsável por transformar texto bruto em tokens vetorizados.\n",
    "\n",
    "# Importa o decorador dataclass, que permite criar classes simples para armazenar configurações ou dados.\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Importa bibliotecas auxiliares:\n",
    "import pandas as pd       # Biblioteca para análise e manipulação de dados, especialmente tabelas e DataFrames.\n",
    "import numpy as np        # Biblioteca para computação numérica eficiente, trabalhando com arrays e operações vetoriais.\n",
    "import glob               # Usada para buscar arquivos usando padrões no nome (ex: *.txt, *.csv).\n",
    "import re                 # Usada para aplicar expressões regulares, muito útil em pré-processamento de texto.\n",
    "from pprint import pprint # Usada para imprimir objetos Python de forma mais legível (útil para debugging).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df65af3-66ec-4a3d-ae8b-1aef78421b9c",
   "metadata": {},
   "source": [
    "## Setup de Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7905bfec-d6ec-4b7a-bca4-f382c6dea7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# Define uma classe de configuração usando o decorador @dataclass.\n",
    "# Essa estrutura é útil para armazenar todos os hiperparâmetros do modelo de forma organizada.\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Tamanho máximo de tokens por sequência (limite de comprimento das frases para entrada no modelo)\n",
    "    MAX_LEN = 256\n",
    "\n",
    "    # Tamanho do lote (batch) utilizado durante o treinamento do modelo\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    # Taxa de aprendizado (learning rate) usada pelo otimizador para ajustar os pesos do modelo\n",
    "    LR = 0.001\n",
    "\n",
    "    # Tamanho do vocabulário (número de tokens únicos que o modelo consegue representar)\n",
    "    VOCAB_SIZE = 30000\n",
    "\n",
    "    # Dimensão dos vetores de embedding e também da saída das atenções\n",
    "    EMBED_DIM = 128\n",
    "\n",
    "    # Número de cabeças na camada de atenção multi-cabeça (Multi-Head Attention)\n",
    "    NUM_HEAD = 8\n",
    "\n",
    "    # Dimensão intermediária da rede feedforward dentro dos blocos Transformer\n",
    "    FF_DIM = 128\n",
    "\n",
    "    # Número de blocos Transformer empilhados (equivale ao número de camadas no encoder)\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "# Cria uma instância da configuração que pode ser acessada em todo o script.\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49351e46-71e2-4a1c-9d89-9f9e6a1886a1",
   "metadata": {},
   "source": [
    "## Download e Descompactação do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b607e8d-74db-411b-8bbd-755edf0b2880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# URL do dataset IMDB de análise de sentimentos (positivo/negativo), em formato compactado .tar.gz\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# Nome do arquivo que será salvo localmente após o download\n",
    "filename = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "# Faz o download do arquivo a partir da URL e salva com o nome especificado\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Abre o arquivo compactado .tar.gz e extrai seu conteúdo no diretório atual\n",
    "with tarfile.open(filename, \"r:gz\") as tar:\n",
    "    tar.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052936f5-2a3a-42c6-ae9a-adc82d903f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que recebe uma lista de arquivos de texto e retorna uma lista com todas as linhas de todos os arquivos.\n",
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)  # Adiciona cada linha do arquivo na lista\n",
    "    return text_list\n",
    "\n",
    "# Função que carrega textos e rótulos de sentimento de uma pasta (train ou test) do dataset IMDB\n",
    "def get_data_from_text_files(folder_name):\n",
    "    # Coleta os caminhos dos arquivos de reviews positivas\n",
    "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    pos_texts = get_text_list_from_files(pos_files)\n",
    "\n",
    "    # Coleta os caminhos dos arquivos de reviews negativas\n",
    "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_files(neg_files)\n",
    "\n",
    "    # Cria um DataFrame com os textos e os respectivos rótulos:\n",
    "    # 0 para positivo, 1 para negativo (inverso do comum, cuidado!)\n",
    "    df = pd.DataFrame({\n",
    "        \"review\": pos_texts + neg_texts,\n",
    "        \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
    "    })\n",
    "\n",
    "    # Embaralha o DataFrame para evitar viés na ordem\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Carrega e prepara os conjuntos de treino e teste\n",
    "train_df = get_data_from_text_files(\"train\")\n",
    "test_df = get_data_from_text_files(\"test\")\n",
    "\n",
    "# Junta todos os dados (útil para criar vocabulário ou análises globais)\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670b058-a694-4a53-a618-9b6d2ec61625",
   "metadata": {},
   "source": [
    "## Preparação do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a4e5169-8648-4122-915f-b7fce4267137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 02:46:25.186114: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-04-20 02:46:25.186198: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-20 02:46:25.186211: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-20 02:46:25.186252: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-20 02:46:25.186276: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Função de normalização personalizada para o TextVectorization\n",
    "# - Converte para minúsculas\n",
    "# - Remove tags HTML como <br />\n",
    "# - Remove pontuações específicas\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
    "    )\n",
    "\n",
    "# Função para criar e configurar a camada de vetorizar texto do Keras\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"\n",
    "    Cria uma camada de vetorizar texto (TextVectorization) com tokenização customizada.\n",
    "\n",
    "    Args:\n",
    "      texts (list): Lista de strings com os textos de entrada\n",
    "      vocab_size (int): Tamanho máximo do vocabulário\n",
    "      max_seq (int): Tamanho máximo de sequência\n",
    "      special_tokens (list): Tokens especiais que devem ser incluídos no vocabulário\n",
    "\n",
    "    Returns:\n",
    "      TextVectorization: Camada configurada e adaptada ao vocabulário\n",
    "    \"\"\"\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)  # Aprende o vocabulário a partir dos dados\n",
    "\n",
    "    # Insere token especial [MASK] manualmente no vocabulário\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "# Cria a camada de vetorizar texto com o vocabulário aprendido dos dados\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    all_data.review.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Obtém o ID correspondente ao token [MASK]\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
    "\n",
    "# Função para codificar textos em sequências de inteiros\n",
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()\n",
    "\n",
    "# Função que aplica o esquema de mascaramento BERT:\n",
    "# 15% dos tokens são candidatos a serem mascarados, com:\n",
    "# - 90% substituídos por [MASK]\n",
    "# - 10% substituídos por tokens aleatórios\n",
    "# - Os restantes permanecem os mesmos\n",
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15  # Seleciona 15% para masking\n",
    "    inp_mask[encoded_texts <= 2] = False  # Evita mascarar tokens especiais (padding, start, etc.)\n",
    "\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)  # Inicializa com -1 (ignorar)\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]  # Salva os verdadeiros rótulos dos tokens mascarados\n",
    "\n",
    "    # Aplica o token [MASK] em 90% dos tokens selecionados para masking\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[inp_mask_2mask] = mask_token_id\n",
    "\n",
    "    # Substitui 10% por tokens aleatórios (entre 3 e o último ID do vocabulário)\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Define pesos das amostras (tokens) - só tokens com label válido contribuem para o loss\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # Retorna inputs com masking, rótulos originais e pesos\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "    return encoded_texts_masked, y_labels, sample_weights\n",
    "\n",
    "# Pré-processamento dos dados de treino para o classificador de sentimentos\n",
    "x_train = encode(train_df.review.values)  # Transforma texto em sequências de IDs\n",
    "y_train = train_df.sentiment.values\n",
    "train_classifier_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# Pré-processamento dos dados de teste\n",
    "x_test = encode(test_df.review.values)\n",
    "y_test = test_df.sentiment.values\n",
    "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
    "    config.BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Armazena o DataFrame bruto de teste para possível uso posterior no modelo end-to-end\n",
    "test_raw_classifier_ds = test_df\n",
    "\n",
    "# Gera dados de entrada para treinamento com Masked Language Modeling (MLM)\n",
    "x_all_review = encode(all_data.review.values)\n",
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "    x_all_review\n",
    ")\n",
    "\n",
    "# Cria dataset TensorFlow para o treinamento do modelo de linguagem\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_train, y_masked_labels, sample_weights)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ab757-10ae-4f39-9d5d-71b68cfeee4f",
   "metadata": {},
   "source": [
    "## Criando o modelo BERT (Modelo de Pré-treinamento) para MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371953c0-412d-4ba5-9e57-51132f108cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"masked_bert_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"masked_bert_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\"> Param # </span>┃<span style=\"font-weight: bold\"> Connected to         </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">960,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ position_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │   <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>) │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │         │ position_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_multihea… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │   <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │         │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_att_drop… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_multihead… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                     │                   │         │ encoder_0_att_dropo… │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_att_laye… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_ffn       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │   <span style=\"color: #00af00; text-decoration-color: #00af00\">8,352</span> │ encoder_0_att_layer… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_ffn_drop… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_ffn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_att_layer… │\n",
       "│                     │                   │         │ encoder_0_ffn_dropo… │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_ffn_laye… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ mlm_cls (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">990,000</span> │ encoder_0_ffn_layer… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)            │         │                      │\n",
       "└─────────────────────┴───────────────────┴─────────┴──────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mParam #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m0\u001b[0m │ -                    │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ word_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │ \u001b[38;5;34m960,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ position_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │   \u001b[38;5;34m4,096\u001b[0m │ word_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mPositionEmbedding\u001b[0m) │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │       \u001b[38;5;34m0\u001b[0m │ word_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │         │ position_embedding[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_multihea… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │   \u001b[38;5;34m4,224\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │         │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_att_drop… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │       \u001b[38;5;34m0\u001b[0m │ encoder_0_multihead… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │       \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                     │                   │         │ encoder_0_att_dropo… │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_att_laye… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │      \u001b[38;5;34m64\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_ffn       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │   \u001b[38;5;34m8,352\u001b[0m │ encoder_0_att_layer… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_ffn_drop… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │       \u001b[38;5;34m0\u001b[0m │ encoder_0_ffn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │       \u001b[38;5;34m0\u001b[0m │ encoder_0_att_layer… │\n",
       "│                     │                   │         │ encoder_0_ffn_dropo… │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ encoder_0_ffn_laye… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │      \u001b[38;5;34m64\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ mlm_cls (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m,       │ \u001b[38;5;34m990,000\u001b[0m │ encoder_0_ffn_layer… │\n",
       "│                     │ \u001b[38;5;34m30000\u001b[0m)            │         │                      │\n",
       "└─────────────────────┴───────────────────┴─────────┴──────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,966,800</span> (7.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,966,800\u001b[0m (7.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,966,800</span> (7.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,966,800\u001b[0m (7.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras_nlp\n",
    "\n",
    "# Função que define um bloco encoder do BERT (atenção + feedforward)\n",
    "def bert_module(query, key, value, i):\n",
    "    # Camada de Atenção Multi-Cabeça (self-attention)\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,  # divisão da dimensão entre as cabeças\n",
    "        name=f\"encoder_{i}_multiheadattention\"\n",
    "    )(query, key, value)\n",
    "\n",
    "    # Dropout após a atenção\n",
    "    attention_output = layers.Dropout(0.1, name=f\"encoder_{i}_att_dropout\")(attention_output)\n",
    "\n",
    "    # Normalização + conexão residual\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6,\n",
    "        name=f\"encoder_{i}_att_layernormalization\"\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Camada FeedForward: Dense → ReLU → Dense\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=f\"encoder_{i}_ffn\"\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=f\"encoder_{i}_ffn_dropout\")(ffn_output)\n",
    "\n",
    "    # Normalização + conexão residual após feedforward\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6,\n",
    "        name=f\"encoder_{i}_ffn_layernormalization\"\n",
    "    )(attention_output + ffn_output)\n",
    "\n",
    "    return sequence_output\n",
    "\n",
    "# Função de perda para MLM: Entropia cruzada com pesos por token\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(reduction=None)\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "# Modelo customizado para linguagem com máscara (MLM)\n",
    "class MaskedLanguageModel(keras.Model):\n",
    "\n",
    "    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n",
    "        # Calcula a perda por token e atualiza a métrica de loss\n",
    "        loss = loss_fn(y, y_pred, sample_weight)\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "        return keras.ops.sum(loss)\n",
    "\n",
    "    def compute_metrics(self, x, y, y_pred, sample_weight):\n",
    "        # Retorna as métricas monitoradas (loss médio da época)\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # Necessário para o reset automático das métricas\n",
    "        return [loss_tracker]\n",
    "\n",
    "# Função que constrói o modelo de linguagem BERT-like com predição de tokens mascarados\n",
    "def create_masked_language_bert_model():\n",
    "    # Entrada: sequência de inteiros (IDs dos tokens)\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")\n",
    "\n",
    "    # Embedding de palavras e de posições (keras-nlp)\n",
    "    word_embeddings = layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\")(inputs)\n",
    "    position_embeddings = keras_nlp.layers.PositionEmbedding(\n",
    "        sequence_length=config.MAX_LEN,\n",
    "        name=\"position_embedding\"\n",
    "    )(word_embeddings)\n",
    "\n",
    "    embeddings = word_embeddings + position_embeddings  # Soma embeddings + posição\n",
    "\n",
    "    # Passa por blocos Transformer empilhados\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    # Camada final de predição com softmax sobre o vocabulário\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(encoder_output)\n",
    "\n",
    "    # Modelo final\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "# Dicionários de conversão entre ID e token\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}\n",
    "\n",
    "# Callback que gera e imprime predições para tokens mascarados ao final de cada época\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        # Converte uma sequência de IDs em texto\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Faz predição usando o modelo\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        # Localiza o índice do token [MASK] na sequência\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]  # Posição na sequência\n",
    "\n",
    "        # Extrai a predição correspondente ao token [MASK]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        # Pega os top-k tokens mais prováveis para preencher a máscara\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "# Exemplo de frase com [MASK] para o callback\n",
    "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
    "\n",
    "# Cria o modelo final\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "\n",
    "# Mostra o resumo da arquitetura do modelo\n",
    "bert_masked_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3dd022-37d0-45dd-b453-43ced9265035",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9720651a-6893-404f-bd28-52de6dad258d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 02:46:43.353569: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 751ms/step\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.04429853}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.039971013}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.03976579}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.029406937}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'is',\n",
      " 'prediction': 'i have watched this is and it was awesome',\n",
      " 'probability': 0.024159651}\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 222ms/step - loss: 7.4814\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.07455514}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.063378215}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.049253553}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.031664167}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was awesome',\n",
      " 'probability': 0.026994266}\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 223ms/step - loss: 6.6597\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.081385605}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.06819729}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.046800464}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.032479517}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was awesome',\n",
      " 'probability': 0.02921724}\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 223ms/step - loss: 6.3867\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.10810452}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.09457294}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.05346705}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.04560975}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'was',\n",
      " 'prediction': 'i have watched this was and it was awesome',\n",
      " 'probability': 0.03344214}\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 223ms/step - loss: 6.1893\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.10058179}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.070160165}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.044865604}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.041760877}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.032062966}\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 225ms/step - loss: 6.0515\n"
     ]
    }
   ],
   "source": [
    "# Treinamento do modelo BERT com Masked Language Modeling por 5 épocas.\n",
    "# Durante o treinamento, o callback irá gerar predições para o token [MASK] ao final de cada época.\n",
    "bert_masked_model.fit(\n",
    "    mlm_ds,\n",
    "    epochs=5,\n",
    "    callbacks=[generator_callback]  # Callback para visualizar a predição do [MASK]\n",
    ")\n",
    "\n",
    "# Salva o modelo treinado no formato .keras (padrão moderno recomendado pelo Keras)\n",
    "bert_masked_model.save(\"bert_mlm_imdb.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8ff50-eee3-4041-bd14-1ea2bf6256ee",
   "metadata": {},
   "source": [
    "## Fine-tune de um modelo de classificação de sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e08829-2a99-4d83-8e2d-9c5a6be3477d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"classification\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"classification\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ functional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">976,800</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ functional_4 (\u001b[38;5;33mFunctional\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │    \u001b[38;5;34m976,800\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │          \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │      \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │         \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">978,977</span> (3.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m978,977\u001b[0m (3.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,177</span> (8.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,177\u001b[0m (8.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">976,800</span> (3.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m976,800\u001b[0m (3.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 35ms/step - accuracy: 0.5701 - loss: 0.7559 - val_accuracy: 0.6478 - val_loss: 0.6274\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.6427 - loss: 0.6345 - val_accuracy: 0.6511 - val_loss: 0.6242\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.6446 - loss: 0.6316 - val_accuracy: 0.6598 - val_loss: 0.6172\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.6545 - loss: 0.6253 - val_accuracy: 0.6440 - val_loss: 0.6311\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.6444 - loss: 0.6282 - val_accuracy: 0.6383 - val_loss: 0.6357\n",
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 61ms/step - accuracy: 0.7431 - loss: 0.5124 - val_accuracy: 0.8279 - val_loss: 0.3802\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - accuracy: 0.8873 - loss: 0.2677 - val_accuracy: 0.8127 - val_loss: 0.4448\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - accuracy: 0.9507 - loss: 0.1402 - val_accuracy: 0.8174 - val_loss: 0.5623\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - accuracy: 0.9814 - loss: 0.0595 - val_accuracy: 0.8044 - val_loss: 0.7739\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - accuracy: 0.9950 - loss: 0.0191 - val_accuracy: 0.8047 - val_loss: 0.9638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x30b63ff40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega o modelo BERT pré-treinado com masked language modeling\n",
    "# É necessário passar a classe customizada usada na definição original (MaskedLanguageModel)\n",
    "mlm_model = keras.models.load_model(\n",
    "    \"bert_mlm_imdb.keras\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\n",
    "\n",
    "# Extrai apenas a parte do encoder BERT (até a última normalização do encoder)\n",
    "# Isso será reutilizado como extrator de features no modelo de classificação\n",
    "pretrained_bert_model = keras.Model(\n",
    "    mlm_model.input,\n",
    "    mlm_model.get_layer(\"encoder_0_ffn_layernormalization\").output\n",
    ")\n",
    "\n",
    "# Congela os pesos do encoder BERT para treinar só as camadas do classificador\n",
    "pretrained_bert_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fd04c-ad38-47b9-b7b8-39614408d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que cria o modelo de classificação com base no encoder BERT já treinado\n",
    "def create_classifier_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")\n",
    "    sequence_output = pretrained_bert_model(inputs)  # Saída do encoder BERT\n",
    "\n",
    "    # Aplica Global Max Pooling para condensar a sequência em um vetor fixo\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "\n",
    "    # Camada densa intermediária\n",
    "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
    "\n",
    "    # Camada de saída binária com ativação sigmoide (classificação de sentimento)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    # Define e compila o modelo\n",
    "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    classifer_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",  # Saída binária (positivo/negativo)\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return classifer_model\n",
    "\n",
    "# Cria o modelo de classificação\n",
    "classifer_model = create_classifier_bert_model()\n",
    "classifer_model.summary()  # Exibe a arquitetura do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29434647-3220-4e21-9406-82fc8aeeae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina o classificador com o encoder BERT congelado (apenas a parte final será ajustada)\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_classifier_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ec01b-b8d9-4c1a-a97f-4e27b0620dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descongela o encoder BERT para realizar fine-tuning completo\n",
    "pretrained_bert_model.trainable = True\n",
    "\n",
    "# Recompila o modelo com novo otimizador\n",
    "optimizer = keras.optimizers.Adam()\n",
    "classifer_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Realiza novo treinamento agora ajustando toda a arquitetura, incluindo o encoder\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_classifier_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb16d8c1-3891-4019-bbea-e2772442ee3b",
   "metadata": {},
   "source": [
    "## Modelo de ponta a ponta que incorpore a TextVectorizationcamada dentro do método evalaute e avaliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15a15350-b478-4730-b77e-789c8f44dcf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.8023 - loss: 0.9687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9634618163108826, 0.8046675324440002]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria uma subclasse de keras.Model para avaliar diretamente um DataFrame com texto cru\n",
    "# Essa abordagem permite usar `.evaluate()` com um DataFrame (em vez de tensor pré-processado)\n",
    "class ModelEndtoEnd(keras.Model):\n",
    "    \n",
    "    def evaluate(self, inputs):\n",
    "        # Pré-processa os dados de entrada (conversão de texto para IDs)\n",
    "        features = encode(inputs.review.values)\n",
    "        labels = inputs.sentiment.values\n",
    "        \n",
    "        # Cria um dataset pronto para avaliação\n",
    "        test_classifier_ds = (\n",
    "            tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "            .shuffle(1000)\n",
    "            .batch(config.BATCH_SIZE)\n",
    "        )\n",
    "        \n",
    "        # Chama o método de avaliação padrão da superclasse\n",
    "        return super().evaluate(test_classifier_ds)\n",
    "\n",
    "    # Método necessário para compatibilidade com a API do Keras\n",
    "    def build(self, input_shape):\n",
    "        self.built = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0652ec-ce79-4265-83ec-9433cf59e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsula o modelo classificador em uma versão end-to-end,\n",
    "# que aceita diretamente um DataFrame com texto cru (review, sentiment)\n",
    "def get_end_to_end(model):\n",
    "    # Reutiliza entradas e saídas do classificador treinado\n",
    "    inputs = classifer_model.inputs[0]\n",
    "    outputs = classifer_model.outputs\n",
    "\n",
    "    # Cria o novo modelo baseado na classe customizada\n",
    "    end_to_end_model = ModelEndtoEnd(inputs, outputs, name=\"end_to_end_model\")\n",
    "\n",
    "    # Compila o modelo com o mesmo otimizador e métricas\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return end_to_end_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daeca9c-d46a-4bb9-9cac-34a1ec9f7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o modelo final que aceita DataFrame com texto direto\n",
    "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
    "\n",
    "# Avalia diretamente o DataFrame bruto (sem precisar codificar manualmente)\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be3524-1aee-4ee2-94d3-c749fe5cd738",
   "metadata": {},
   "source": [
    "# Modelagem de Linguagem Mascarada de Ponta a Ponta com BERT - Dataset Short Jokes (Piadas Curtas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5485b-3acc-4d84-9e7d-0a9e239a18c0",
   "metadata": {},
   "source": [
    "## Importações e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070f4999-2a8d-4c7e-afaf-3e6a12a823ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83a6d69-91cc-4dae-aa9b-1a8db29fe0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 128\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 32\n",
    "    NUM_HEAD = 4\n",
    "    FF_DIM = 128\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e4a35-3eaa-4060-b501-1fd1b886d332",
   "metadata": {},
   "source": [
    "## Carregamento e Amostragem do Dataset Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae10cc03-37a2-44e8-977d-bf07a7a31f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649518730f684321b9e548620f98d542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d4d5475229418da548b94538374de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00004.parquet:   0%|          | 0.00/260M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0822d1570e9f43a9990aa0272e0082a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00004.parquet:   0%|          | 0.00/258M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3492ba8000f2428b9de26df3a1379177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00004.parquet:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307c7c44ba3f40539c03c22187ebcb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00003-of-00004.parquet:   0%|          | 0.00/254M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065c1fc625664393bfe603aea06e36d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f37dc976334bb2b98a2b40dfadfbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5549557ef69a431eaefc7764dc9b41fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/400000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"amazon_polarity\")\n",
    "train_ds = dataset[\"train\"].shuffle(seed=42).select(range(40000))\n",
    "test_ds = dataset[\"test\"].shuffle(seed=42).select(range(10000))\n",
    "\n",
    "train_df = pd.DataFrame({\"review\": train_ds[\"content\"], \"sentiment\": train_ds[\"label\"]})\n",
    "test_df = pd.DataFrame({\"review\": test_ds[\"content\"], \"sentiment\": test_ds[\"label\"]})\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac76db-bce5-4ff0-879c-ae41fc63d4a0",
   "metadata": {},
   "source": [
    "## Tokenização e Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02daa50-668e-4f98-873a-f6af7ecd3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 12:02:07.893113: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-04-21 12:02:07.893501: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-21 12:02:07.893539: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-21 12:02:07.893863: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-21 12:02:07.893933: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\\\^_`{|}~\"), \"\")\n",
    "\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    all_data.review.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
    "\n",
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()\n",
    "\n",
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[inp_mask_2mask] = mask_token_id\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(3, mask_token_id, inp_mask_2random.sum())\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7829d88-3130-420e-8b59-cb5c2affc470",
   "metadata": {},
   "source": [
    "## Dataset TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83061dbd-8da2-412d-84ae-c51506ebfacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = encode(train_df.review.values)\n",
    "y_train = train_df.sentiment.values\n",
    "train_classifier_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(config.BATCH_SIZE)\n",
    "\n",
    "x_test = encode(test_df.review.values)\n",
    "y_test = test_df.sentiment.values\n",
    "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(config.BATCH_SIZE)\n",
    "\n",
    "test_raw_classifier_ds = test_df\n",
    "\n",
    "x_all_review = encode(all_data.review.values)\n",
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(x_all_review)\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_train, y_masked_labels, sample_weights)).shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500c89d-1014-41fa-8dd4-3c429f25f348",
   "metadata": {},
   "source": [
    "## Modelo BERT com MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d6248f-d617-48f4-a27a-3ce891157b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_module(query, key, value, i):\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}_multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}_att_dropout\".format(i))(attention_output)\n",
    "    attention_output = layers.LayerNormalization(epsilon=1e-6, name=\"encoder_{}_att_layernormalization\".format(i))(query + attention_output)\n",
    "    ffn = keras.Sequential([\n",
    "        layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "        layers.Dense(config.EMBED_DIM),\n",
    "    ], name=\"encoder_{}_ffn\".format(i))\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}_ffn_dropout\".format(i))(ffn_output)\n",
    "    sequence_output = layers.LayerNormalization(epsilon=1e-6, name=\"encoder_{}_ffn_layernormalization\".format(i))(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(reduction=None)\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "class MaskedLanguageModel(keras.Model):\n",
    "    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n",
    "        loss = loss_fn(y, y_pred, sample_weight)\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "        return keras.ops.sum(loss)\n",
    "    def compute_metrics(self, x, y, y_pred, sample_weight):\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [loss_tracker]\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")\n",
    "    word_embeddings = layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\")(inputs)\n",
    "    position_embeddings = keras_nlp.layers.PositionEmbedding(sequence_length=config.MAX_LEN, name=\"position_embedding\")(word_embeddings)\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(encoder_output)\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a474de-ec03-4e86-b2a7-44b6c039b9ae",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0f6a92-236b-46b4-866a-1daf97349dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 12:02:38.464811: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 237ms/step - loss: 7.4093\n",
      "Epoch 2/5\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 252ms/step - loss: 6.5568\n",
      "Epoch 3/5\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 285ms/step - loss: 6.2563\n",
      "Epoch 4/5\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 269ms/step - loss: 6.0063\n",
      "Epoch 5/5\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 281ms/step - loss: 5.8242\n"
     ]
    }
   ],
   "source": [
    "mlm_model = create_masked_language_bert_model()\n",
    "mlm_model.fit(mlm_ds, epochs=5)\n",
    "mlm_model.save(\"bert_mlm_amazon.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c61ac6-c54e-45a3-a5d5-83354ee0f367",
   "metadata": {},
   "source": [
    "## Classificador binário com BERT congelado e fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf5985f2-f07d-4dea-95c3-c6912532b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step - accuracy: 0.5740 - loss: 0.7276 - val_accuracy: 0.6158 - val_loss: 0.6537\n",
      "Epoch 2/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 25ms/step - accuracy: 0.6090 - loss: 0.6584 - val_accuracy: 0.6210 - val_loss: 0.6499\n",
      "Epoch 3/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 25ms/step - accuracy: 0.6147 - loss: 0.6562 - val_accuracy: 0.5971 - val_loss: 0.6694\n",
      "Epoch 4/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 25ms/step - accuracy: 0.6150 - loss: 0.6561 - val_accuracy: 0.6280 - val_loss: 0.6449\n",
      "Epoch 5/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 25ms/step - accuracy: 0.6190 - loss: 0.6538 - val_accuracy: 0.5593 - val_loss: 0.7134\n",
      "Epoch 1/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.7416 - loss: 0.5062 - val_accuracy: 0.8416 - val_loss: 0.3583\n",
      "Epoch 2/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 62ms/step - accuracy: 0.8706 - loss: 0.3050 - val_accuracy: 0.8431 - val_loss: 0.3606\n",
      "Epoch 3/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 62ms/step - accuracy: 0.9242 - loss: 0.1962 - val_accuracy: 0.8362 - val_loss: 0.4273\n",
      "Epoch 4/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 60ms/step - accuracy: 0.9553 - loss: 0.1201 - val_accuracy: 0.8208 - val_loss: 0.5580\n",
      "Epoch 5/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 63ms/step - accuracy: 0.9771 - loss: 0.0694 - val_accuracy: 0.8198 - val_loss: 0.7099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3b9c6cf70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_model = keras.models.load_model(\"bert_mlm_amazon.keras\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel})\n",
    "pretrained_bert_model = keras.Model(mlm_model.input, mlm_model.get_layer(\"encoder_0_ffn_layernormalization\").output)\n",
    "pretrained_bert_model.trainable = False\n",
    "\n",
    "def create_classifier_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")\n",
    "    sequence_output = pretrained_bert_model(inputs)\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    classifer_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return classifer_model\n",
    "\n",
    "classifer_model = create_classifier_bert_model()\n",
    "classifer_model.fit(train_classifier_ds, epochs=5, validation_data=test_classifier_ds)\n",
    "\n",
    "pretrained_bert_model.trainable = True\n",
    "classifer_model.compile(optimizer=keras.optimizers.Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "classifer_model.fit(train_classifier_ds, epochs=5, validation_data=test_classifier_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7d54f-5ce9-425d-984c-6fe23887e23f",
   "metadata": {},
   "source": [
    "## Avaliação End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b1fbe5e-d49c-4f35-9cfc-cfe82237c3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.8206 - loss: 0.7216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7103360891342163, 0.8195886611938477]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelEndtoEnd(keras.Model):\n",
    "    def evaluate(self, inputs):\n",
    "        features = encode(inputs.review.values)\n",
    "        labels = inputs.sentiment.values\n",
    "        test_classifier_ds = tf.data.Dataset.from_tensor_slices((features, labels)).shuffle(1000).batch(config.BATCH_SIZE)\n",
    "        return super().evaluate(test_classifier_ds)\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "def get_end_to_end(model):\n",
    "    inputs = classifer_model.inputs[0]\n",
    "    outputs = classifer_model.outputs\n",
    "    end_to_end_model = ModelEndtoEnd(inputs, outputs, name=\"end_to_end_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    end_to_end_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return end_to_end_model\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
